{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0359a3f4-0cfe-4ff1-81f8-2d540a252856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c14743f-1dd9-416a-9559-5c31713ef028",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/Users/Gordon Freeman/ml-notebooks/Agricultural-crops'\n",
    "output_folder = '/Users/Gordon Freeman/ml-notebooks/ImageRecognition' # save our file\n",
    "\n",
    "split_ratio = (0.8, 0.1, 0.1) # 80% train, 10% validation, 10% test\n",
    "splitfolders.ratio(\n",
    "    input_folder, \n",
    "    output=output_folder, \n",
    "    seed=500, # random number generator, ensures the split is reproducible so running with the same seed means the same split\n",
    "              # order types of ml models use `random-state`\n",
    "    ratio=split_ratio, \n",
    "    group_prefix=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085a17fd-27df-4218-b922-00ca43b3338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224) # resize the images to 224x224 pixels, this is a common size usef for deep learning\n",
    "batch_size = 32       # models weight is updated after it processes 32 images\n",
    "\n",
    "# data augmentation for the training data to expand the dataset with transformed versions, improves model generalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input, # resnet50 pre trained model\n",
    "    rotation_range=20,                       # randomly rotate the image by up to 20 degrees\n",
    "    width_shift_range=0.2,                   # randomly shift the image horizontally left/right by up to 20% of the width\n",
    "    height_shift_range=0.2,                  # randomly shift the image vertically up/down by up to 20% of the height\n",
    "    shear_range=0.2,                         # random shear transformations up to 20%\n",
    "    zoom_range=0.2,                          # randomly zooms into the image up to 20%\n",
    "    horizontal_flip=True,                    # randomly flips the image\n",
    "    fill_mode='nearest'                      # when the image is rotated/shifted and a new pixel needs to be filled in, the nearest is used\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # data augmentation for test data (only rescaling)\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) # data augmentation for validation data (only rescaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8625fe7a-6a19-4e4f-8fae-2d3a77f95cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 652 images belonging to 30 classes.\n",
      "Found 105 images belonging to 30 classes.\n",
      "Found 105 images belonging to 30 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(output_folder, 'train')\n",
    "test_dir = os.path.join(output_folder, 'test')\n",
    "val_dir = os.path.join(output_folder, 'val')\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical' # type of label array to be returned, categorical means the labels will be one hot encoded, useful for multiclass classification\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_data = valid_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40dba865-e081-4dc1-a5e0-2cd2c1e23949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet import ResNet50 # Convolutional Neural Networks (CNN) that has been pre-trained on images\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',                       # use weights of the model that has been pre-trained\n",
    "    include_top=False,                        # dont include the fully connected layers at the top of the network\n",
    "                                              # the top refers to the classification layers that are normally at the end of our network\n",
    "                                              # by excluding this we can add our own custom classification layers suitable for our problem\n",
    "    input_shape=(img_size[0], img_size[1], 3) # shape of input images, they are expected 224 by 224px with 3 colour channels RGB (red/green/blue)\n",
    ")\n",
    "\n",
    "base_model.trainable=False # freeze convolutional base, meaning the weights of these layers will not be updated during training\n",
    "                           # done to preserve the pre-trained weights and only train the newly added classification layers\n",
    "                           # freezing the base model helps to leverage the features learnt from pre training without altering them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bd3567-1245-4c1b-8820-d2343a0fdc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,                            # pre trained ResNet50 model\n",
    "    layers.GlobalAveragePooling2D(),       # used to replace fully connected layers in CNNs to reduce overfitting and the number of parameters\n",
    "    layers.Dense(128, activation='relu'),  # fully connected dense layer with 128 neurons, relu (rectified linear unit) is the activation function\n",
    "                                           # relu introduces nonlinearity enabling the model to learn more complex representations\n",
    "    layers.Dropout(0.5),                   # randomly sets 50% of its input units to zero during each update\n",
    "                                           # this also helps to prevent overfitting\n",
    "    layers.Dense(30, activation='softmax') # another fully connected layer with 30 neurons\n",
    "                                           # softmax activation function transforms raw output scores (logits) into a probability distribution\n",
    "                                           # \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8947914-f7cd-4bb8-b3c8-da76a4b1143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',                # updates the models weight during training to minimize the loss function\n",
    "                                     # `adam` (adaotive moment estimation) is an advanced gradient descent algorithm that adjusts the learning rate for each parameter \n",
    "                                     # this done based on estimates for lower order moments\n",
    "                                     # other optimizers exist but `adam` is widely used because its computationally efficient and requires less memory\n",
    "\n",
    "    loss='categorical_crossentropy', # loss functions in ml measure how well the models prediction matches the true actual values\n",
    "                                     # during training the optimizer tries to minimize the loss\n",
    "                                     # other loss functions like `Mean Squared Error (MSE)` exist\n",
    "\n",
    "    metrics=['accuracy']             # metrics evaluate the performance of the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13f9f082-bf5b-401b-91e8-e1ca83857140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.4839 - loss: 1.8064 - val_accuracy: 0.5619 - val_loss: 1.4423\n",
      "Epoch 2/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.5370 - loss: 1.5586 - val_accuracy: 0.6381 - val_loss: 1.3040\n",
      "Epoch 3/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.5619 - loss: 1.4978 - val_accuracy: 0.6571 - val_loss: 1.2561\n",
      "Epoch 4/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.5919 - loss: 1.4334 - val_accuracy: 0.6762 - val_loss: 1.1420\n",
      "Epoch 5/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.5974 - loss: 1.2830 - val_accuracy: 0.6667 - val_loss: 1.0668\n",
      "Epoch 6/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.6544 - loss: 1.1018 - val_accuracy: 0.7048 - val_loss: 1.0384\n",
      "Epoch 7/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.6968 - loss: 1.1063 - val_accuracy: 0.7238 - val_loss: 0.9744\n",
      "Epoch 8/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.6849 - loss: 1.1422 - val_accuracy: 0.7143 - val_loss: 0.9487\n",
      "Epoch 9/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.6991 - loss: 1.0302 - val_accuracy: 0.6952 - val_loss: 0.9326\n",
      "Epoch 10/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7170 - loss: 0.9645 - val_accuracy: 0.7333 - val_loss: 0.8958\n",
      "Epoch 11/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7431 - loss: 0.9488 - val_accuracy: 0.7238 - val_loss: 0.8788\n",
      "Epoch 12/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.7268 - loss: 0.8667 - val_accuracy: 0.7524 - val_loss: 0.8527\n",
      "Epoch 13/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7634 - loss: 0.7790 - val_accuracy: 0.7333 - val_loss: 0.8349\n",
      "Epoch 14/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7563 - loss: 0.7919 - val_accuracy: 0.7810 - val_loss: 0.7962\n",
      "Epoch 15/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7494 - loss: 0.7884 - val_accuracy: 0.7524 - val_loss: 0.8005\n",
      "Epoch 16/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7526 - loss: 0.8171 - val_accuracy: 0.7619 - val_loss: 0.8043\n",
      "Epoch 17/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7635 - loss: 0.7505 - val_accuracy: 0.7714 - val_loss: 0.7931\n",
      "Epoch 18/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7962 - loss: 0.6609 - val_accuracy: 0.7238 - val_loss: 0.8330\n",
      "Epoch 19/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7759 - loss: 0.6927 - val_accuracy: 0.7714 - val_loss: 0.7783\n",
      "Epoch 20/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8212 - loss: 0.5792 - val_accuracy: 0.7810 - val_loss: 0.7444\n",
      "Epoch 21/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7997 - loss: 0.6302 - val_accuracy: 0.7429 - val_loss: 0.7810\n",
      "Epoch 22/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8176 - loss: 0.5923 - val_accuracy: 0.7524 - val_loss: 0.7909\n",
      "Epoch 23/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.7784 - loss: 0.6889 - val_accuracy: 0.7619 - val_loss: 0.7301\n",
      "Epoch 24/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8343 - loss: 0.5672 - val_accuracy: 0.7714 - val_loss: 0.7596\n",
      "Epoch 25/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8245 - loss: 0.5995 - val_accuracy: 0.7619 - val_loss: 0.7507\n",
      "Epoch 26/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8167 - loss: 0.6024 - val_accuracy: 0.7714 - val_loss: 0.7227\n",
      "Epoch 27/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8397 - loss: 0.5502 - val_accuracy: 0.8000 - val_loss: 0.7122\n",
      "Epoch 28/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8260 - loss: 0.5532 - val_accuracy: 0.7524 - val_loss: 0.6960\n",
      "Epoch 29/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.7985 - loss: 0.5611 - val_accuracy: 0.7619 - val_loss: 0.7272\n",
      "Epoch 30/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8803 - loss: 0.4554 - val_accuracy: 0.7810 - val_loss: 0.7004\n",
      "Epoch 31/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8655 - loss: 0.4373 - val_accuracy: 0.7714 - val_loss: 0.7373\n",
      "Epoch 32/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8570 - loss: 0.4973 - val_accuracy: 0.8000 - val_loss: 0.7053\n",
      "Epoch 33/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.8375 - loss: 0.4875 - val_accuracy: 0.7714 - val_loss: 0.7084\n",
      "Epoch 34/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8954 - loss: 0.3979 - val_accuracy: 0.7238 - val_loss: 0.7502\n",
      "Epoch 35/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8794 - loss: 0.3695 - val_accuracy: 0.7905 - val_loss: 0.7568\n",
      "Epoch 36/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8843 - loss: 0.4008 - val_accuracy: 0.7905 - val_loss: 0.6866\n",
      "Epoch 37/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8545 - loss: 0.4217 - val_accuracy: 0.7524 - val_loss: 0.7604\n",
      "Epoch 38/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8684 - loss: 0.4212 - val_accuracy: 0.7714 - val_loss: 0.6781\n",
      "Epoch 39/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8506 - loss: 0.4774 - val_accuracy: 0.8190 - val_loss: 0.6554\n",
      "Epoch 40/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9069 - loss: 0.3557 - val_accuracy: 0.7905 - val_loss: 0.6784\n",
      "Epoch 41/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8924 - loss: 0.3880 - val_accuracy: 0.7905 - val_loss: 0.6337\n",
      "Epoch 42/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8720 - loss: 0.4104 - val_accuracy: 0.7905 - val_loss: 0.7215\n",
      "Epoch 43/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8766 - loss: 0.3774 - val_accuracy: 0.7714 - val_loss: 0.6717\n",
      "Epoch 44/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8991 - loss: 0.3463 - val_accuracy: 0.7810 - val_loss: 0.6854\n",
      "Epoch 45/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8971 - loss: 0.3897 - val_accuracy: 0.8190 - val_loss: 0.6145\n",
      "Epoch 46/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8720 - loss: 0.3643 - val_accuracy: 0.7905 - val_loss: 0.6968\n",
      "Epoch 47/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8999 - loss: 0.3719 - val_accuracy: 0.8000 - val_loss: 0.6622\n",
      "Epoch 48/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9027 - loss: 0.3343 - val_accuracy: 0.8095 - val_loss: 0.6711\n",
      "Epoch 49/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8969 - loss: 0.3278 - val_accuracy: 0.7810 - val_loss: 0.7271\n",
      "Epoch 50/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9014 - loss: 0.3015 - val_accuracy: 0.8190 - val_loss: 0.6831\n",
      "Epoch 51/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.8867 - loss: 0.3323 - val_accuracy: 0.7905 - val_loss: 0.7619\n",
      "Epoch 52/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8943 - loss: 0.3305 - val_accuracy: 0.8190 - val_loss: 0.7103\n",
      "Epoch 53/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8954 - loss: 0.2977 - val_accuracy: 0.7810 - val_loss: 0.8306\n",
      "Epoch 54/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9362 - loss: 0.2554 - val_accuracy: 0.7810 - val_loss: 0.7146\n",
      "Epoch 55/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8965 - loss: 0.2980 - val_accuracy: 0.8286 - val_loss: 0.7426\n",
      "Epoch 56/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.8606 - loss: 0.4048 - val_accuracy: 0.8095 - val_loss: 0.7295\n",
      "Epoch 57/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9058 - loss: 0.3224 - val_accuracy: 0.7810 - val_loss: 0.7276\n",
      "Epoch 58/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9014 - loss: 0.3166 - val_accuracy: 0.7714 - val_loss: 0.7549\n",
      "Epoch 59/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8926 - loss: 0.3167 - val_accuracy: 0.7905 - val_loss: 0.7382\n",
      "Epoch 60/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8910 - loss: 0.3351 - val_accuracy: 0.7810 - val_loss: 0.7539\n",
      "Epoch 61/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9108 - loss: 0.3181 - val_accuracy: 0.8095 - val_loss: 0.7300\n",
      "Epoch 62/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9242 - loss: 0.2679 - val_accuracy: 0.8381 - val_loss: 0.7215\n",
      "Epoch 63/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9156 - loss: 0.2323 - val_accuracy: 0.8286 - val_loss: 0.6978\n",
      "Epoch 64/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9097 - loss: 0.2581 - val_accuracy: 0.8000 - val_loss: 0.7066\n",
      "Epoch 65/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9140 - loss: 0.2494 - val_accuracy: 0.8476 - val_loss: 0.6571\n",
      "Epoch 66/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9050 - loss: 0.2555 - val_accuracy: 0.8476 - val_loss: 0.6626\n",
      "Epoch 67/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9138 - loss: 0.2542 - val_accuracy: 0.8095 - val_loss: 0.7168\n",
      "Epoch 68/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9140 - loss: 0.2855 - val_accuracy: 0.8381 - val_loss: 0.7119\n",
      "Epoch 69/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9036 - loss: 0.2918 - val_accuracy: 0.8095 - val_loss: 0.7033\n",
      "Epoch 70/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.9423 - loss: 0.2325 - val_accuracy: 0.8095 - val_loss: 0.7766\n",
      "Epoch 71/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9132 - loss: 0.2525 - val_accuracy: 0.8095 - val_loss: 0.6606\n",
      "Epoch 72/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.9179 - loss: 0.2425 - val_accuracy: 0.8095 - val_loss: 0.7484\n",
      "Epoch 73/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9243 - loss: 0.2450 - val_accuracy: 0.8286 - val_loss: 0.6993\n",
      "Epoch 74/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9170 - loss: 0.2518 - val_accuracy: 0.8190 - val_loss: 0.6290\n",
      "Epoch 75/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8908 - loss: 0.3123 - val_accuracy: 0.8476 - val_loss: 0.7043\n",
      "Epoch 76/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9232 - loss: 0.2388 - val_accuracy: 0.8381 - val_loss: 0.6758\n",
      "Epoch 77/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9374 - loss: 0.2262 - val_accuracy: 0.8286 - val_loss: 0.6437\n",
      "Epoch 78/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9342 - loss: 0.1963 - val_accuracy: 0.8381 - val_loss: 0.6629\n",
      "Epoch 79/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9296 - loss: 0.2421 - val_accuracy: 0.8095 - val_loss: 0.7001\n",
      "Epoch 80/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9205 - loss: 0.2364 - val_accuracy: 0.7714 - val_loss: 0.7090\n",
      "Epoch 81/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.9156 - loss: 0.2608 - val_accuracy: 0.8190 - val_loss: 0.6565\n",
      "Epoch 82/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9203 - loss: 0.2242 - val_accuracy: 0.8095 - val_loss: 0.7702\n",
      "Epoch 83/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9085 - loss: 0.3133 - val_accuracy: 0.7810 - val_loss: 0.6924\n",
      "Epoch 84/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9117 - loss: 0.2577 - val_accuracy: 0.7905 - val_loss: 0.7317\n",
      "Epoch 85/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9218 - loss: 0.2326 - val_accuracy: 0.8190 - val_loss: 0.7101\n",
      "Epoch 86/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9245 - loss: 0.2425 - val_accuracy: 0.8190 - val_loss: 0.8058\n",
      "Epoch 87/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9040 - loss: 0.2804 - val_accuracy: 0.8095 - val_loss: 0.6626\n",
      "Epoch 88/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8946 - loss: 0.2671 - val_accuracy: 0.8095 - val_loss: 0.7280\n",
      "Epoch 89/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8876 - loss: 0.3534 - val_accuracy: 0.8000 - val_loss: 0.7700\n",
      "Epoch 90/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9200 - loss: 0.2306 - val_accuracy: 0.8000 - val_loss: 0.7865\n",
      "Epoch 91/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9172 - loss: 0.2404 - val_accuracy: 0.8000 - val_loss: 0.8206\n",
      "Epoch 92/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9199 - loss: 0.2421 - val_accuracy: 0.8000 - val_loss: 0.7819\n",
      "Epoch 93/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9267 - loss: 0.2172 - val_accuracy: 0.8190 - val_loss: 0.7928\n",
      "Epoch 94/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9493 - loss: 0.1905 - val_accuracy: 0.8190 - val_loss: 0.7600\n",
      "Epoch 95/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9389 - loss: 0.1885 - val_accuracy: 0.8381 - val_loss: 0.6928\n",
      "Epoch 96/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9507 - loss: 0.1751 - val_accuracy: 0.8286 - val_loss: 0.6828\n",
      "Epoch 97/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.9301 - loss: 0.2298 - val_accuracy: 0.8286 - val_loss: 0.6413\n",
      "Epoch 98/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9132 - loss: 0.2576 - val_accuracy: 0.8286 - val_loss: 0.6380\n",
      "Epoch 99/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9293 - loss: 0.2140 - val_accuracy: 0.8286 - val_loss: 0.6728\n",
      "Epoch 100/100\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9256 - loss: 0.2145 - val_accuracy: 0.8190 - val_loss: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2255c60ee60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=100,                # epoch is one complete pass though the training data set, so 100 means do it 100 times (all batches)\n",
    "                               # 100 is very common in examples (should take about 30 minutes on a standard desktop machine)\n",
    "                               # the idea is with each epoch of time, the `accuracy` goes up and the `loss` goes down\n",
    "    validation_data=valid_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11bfc2af-f72f-4b57-813c-9ff795636906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.8078 - loss: 0.8704\n",
      "Test Accuracy: 81.90%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05a8f2a-7fed-4ff7-8cea-95864bb386da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'Cherry',\n",
    "    1: 'Coffee-plant',\n",
    "    2: 'Cucumber',\n",
    "    3: 'Fox_nut(Makhana)',\n",
    "    4: 'Lemon',\n",
    "    5: 'Olive-tree',\n",
    "    6: 'Pearl_millet(bajra)',\n",
    "    7: 'Tobacco-plant',\n",
    "    8: 'almond',\n",
    "    9: 'banana',\n",
    "    10: 'cardamom',\n",
    "    11: 'chilli',\n",
    "    12: 'clove',\n",
    "    13: 'coconut',\n",
    "    14: 'cotton',\n",
    "    15: 'gram',\n",
    "    16: 'jowar',\n",
    "    17: 'jute',\n",
    "    18: 'maize',\n",
    "    19: 'mustard-oil',\n",
    "    20: 'papaya',\n",
    "    21: 'pineapple',\n",
    "    22: 'rice',\n",
    "    23: 'soyabean',\n",
    "    24: 'sugarcane',\n",
    "    25: 'sunflower',\n",
    "    26: 'tea',\n",
    "    27: 'tomato',\n",
    "    28: 'vigna-radiati(Mung)',\n",
    "    29: 'wheat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bfaceed-bd55-40a9-b24d-9bbc7016cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(image, model):\n",
    "    test_img=cv2.imread(image)                # read the image from the specified file path as an array\n",
    "    test_img=cv2.resize(test_img, (224,224))  # resize to 224 by 224px to match the size the model was trained on\n",
    "    test_img=np.expand_dims(test_img, axis=0) # numpy function to add extra dimensions to the image array\n",
    "    result=model.predict(test_img)            # use trained model to make prediction\n",
    "    r=np.argmax(result)                       # returns the index of the maxium value in the result array, \n",
    "                                              # this should correspond to the class with the highest probability\n",
    "    print(class_names[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac94861b-3cec-4926-afa9-56be34a621b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Pearl_millet(bajra)\n"
     ]
    }
   ],
   "source": [
    "predict_img(\n",
    "    '/Users/Gordon Freeman/ml-notebooks/Agricultural-crops/Pearl_millet(bajra)/image (50).jpg',\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75c3ef79-0874-4ca1-9d6e-48890a7b6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Agricultural-crops.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048360df-652b-452a-8708-dbaa31912e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
